{"cells":[{"cell_type":"markdown","source":["# Import Packages"],"metadata":{"id":"gonXNogfDQc0"},"id":"gonXNogfDQc0"},{"cell_type":"code","execution_count":null,"id":"f61e4d96","metadata":{"id":"f61e4d96"},"outputs":[],"source":["import pandas            as pd\n","import numpy             as np\n","import matplotlib.pyplot as plt\n","import seaborn           as sb\n","import scipy.stats       as sp\n","import tensorflow        as tf\n","from tensorflow                 import keras\n","from keras.models    import Model\n","from sklearn.model_selection    import train_test_split"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4N7DljWGdH-g"},"id":"4N7DljWGdH-g","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Dataset for Visualization"],"metadata":{"id":"c1LCWrpRDY4R"},"id":"c1LCWrpRDY4R"},{"cell_type":"code","source":["# Define each condition's name and number of samples\n","cond = ['Tool Change', 'Chip Conveyer', 'Moving X axis', 'Moving Y axis', 'Moving Z axis', 'Spindle Movement']\n","\n","NoOfData = pd.DataFrame([96, 161, 144, 41, 34, 147])\n","NoOfData"],"metadata":{"id":"JRY_79DyfHXo"},"id":"JRY_79DyfHXo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load STFT dataset that you saved in ML8\n","DataSet = np.load('/content/drive/MyDrive/SoundSTFT.npy')[:,:,:-1]\n","DataSet.shape"],"metadata":{"id":"7OflYTAoeiA6"},"id":"7OflYTAoeiA6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the width and height of each STFT spectrogram\n","width, height = DataSet.shape[1], DataSet.shape[2]\n","width, height"],"metadata":{"id":"FGU3Ok7LjNJQ"},"id":"FGU3Ok7LjNJQ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"814ffa2b","metadata":{"id":"814ffa2b"},"outputs":[],"source":["# Scale the Dataset\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from joblib import dump, load\n","\n","DataSet_2d = DataSet.reshape(DataSet.shape[0],-1)\n","print(DataSet_2d.shape)\n","\n","scaler_std = StandardScaler()\n","scaler_mMx = MinMaxScaler()\n","DataSet_2d_scaled_1 = scaler_std.fit_transform(DataSet_2d)\n","DataSet_2d_scaled_2 = scaler_mMx.fit_transform(DataSet_2d_scaled_1)\n","\n","DataSet_scaled = DataSet_2d_scaled_2.reshape(DataSet.shape[0], DataSet.shape[1], DataSet.shape[2])\n","print(DataSet_scaled.shape)\n","\n","DataSet_rsp = DataSet_scaled.reshape(DataSet_scaled.shape[0], DataSet_scaled.shape[1], DataSet_scaled.shape[2], 1)\n","print(DataSet_rsp.shape)"]},{"cell_type":"code","execution_count":null,"id":"8242d95c","metadata":{"id":"8242d95c"},"outputs":[],"source":["# Prepare train/test dataset for Conv_AE modeling\n","Train_Test_Ratio = 0.2\n","\n","DataSet_1 = DataSet_rsp[sum(NoOfData.iloc[:0,0]):sum(NoOfData.iloc[:0,0])+NoOfData.iloc[0,0]]\n","DataSet_2 = DataSet_rsp[sum(NoOfData.iloc[:1,0]):sum(NoOfData.iloc[:1,0])+NoOfData.iloc[1,0]]\n","DataSet_3 = DataSet_rsp[sum(NoOfData.iloc[:2,0]):sum(NoOfData.iloc[:2,0])+NoOfData.iloc[2,0]]\n","DataSet_4 = DataSet_rsp[sum(NoOfData.iloc[:3,0]):sum(NoOfData.iloc[:3,0])+NoOfData.iloc[3,0]]\n","DataSet_5 = DataSet_rsp[sum(NoOfData.iloc[:4,0]):sum(NoOfData.iloc[:4,0])+NoOfData.iloc[4,0]]\n","DataSet_6 = DataSet_rsp[sum(NoOfData.iloc[:5,0]):sum(NoOfData.iloc[:5,0])+NoOfData.iloc[5,0]]\n","\n","TrainD1, TestD1, = train_test_split(DataSet_1, test_size=Train_Test_Ratio, random_state = 777)\n","TrainD2, TestD2, = train_test_split(DataSet_2, test_size=Train_Test_Ratio, random_state = 777)\n","TrainD3, TestD3, = train_test_split(DataSet_3, test_size=Train_Test_Ratio, random_state = 777)\n","TrainD4, TestD4, = train_test_split(DataSet_4, test_size=Train_Test_Ratio, random_state = 777)\n","TrainD5, TestD5, = train_test_split(DataSet_5, test_size=Train_Test_Ratio, random_state = 777)\n","TrainD6, TestD6, = train_test_split(DataSet_6, test_size=Train_Test_Ratio, random_state = 777)\n","\n","TrainData  = np.concatenate([TrainD1, TrainD2, TrainD3, TrainD4, TrainD5, TrainD6], axis=0)\n","TestData   = np.concatenate([TestD1 , TestD2 , TestD3 , TestD4, TestD5, TestD6 ], axis=0)\n","\n","TrainData.shape , TestData.shape"]},{"cell_type":"markdown","source":["# Convolutional Autoencoder (Conv_AE) Model Training"],"metadata":{"id":"qWF7XX9DLCfy"},"id":"qWF7XX9DLCfy"},{"cell_type":"code","execution_count":null,"id":"0c63fcb1","metadata":{"id":"0c63fcb1"},"outputs":[],"source":["# Build a class for customized training process confirmation\n","PrintAccPerEpochs = 100\n","\n","class MAE_PerEpoch(keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        keras.callbacks.Callback()\n","        if epoch%PrintAccPerEpochs == 0:\n","            print(\"[{} Epochs] Loss(MAE) : {:.4f} \".format(epoch, logs[\"mae\"]))"]},{"cell_type":"code","execution_count":null,"id":"7f2429ba","metadata":{"id":"7f2429ba"},"outputs":[],"source":["# pytictoc is an useful tool for time calculation\n","!pip install pytictoc\n","from pytictoc import TicToc"]},{"cell_type":"code","execution_count":null,"id":"93edefa3","metadata":{"id":"93edefa3"},"outputs":[],"source":["# Define hyperparameter\n","Epoch = 100\n","\n","DenseUnits_1 = 300\n","DenseUnits_2 = 30"]},{"cell_type":"code","execution_count":null,"id":"3549f43c","metadata":{"id":"3549f43c"},"outputs":[],"source":["t = TicToc()\n","\n","# Design a Conv_AE model\n","################################################################################\n","input = keras.layers.Input(shape=(TrainData.shape[1], TrainData.shape[2], TrainData.shape[3]))\n","\n","# Encoder\n","x = keras.layers.Conv2D(4 , kernel_size=(3,3), padding = 'same', activation = 'selu')(input)\n","x = keras.layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n","x = keras.layers.Conv2D(8 , kernel_size=(3,3), padding = 'same', activation = 'selu')(x)\n","x = keras.layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n","x = keras.layers.Conv2D(16, kernel_size=(3,3), padding = 'same', activation = 'selu')(x)\n","x = keras.layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(units=DenseUnits_1, activation = 'selu')(x)\n","\n","# Bottle neck layer (Latent space)\n","x = keras.layers.Dense(units=DenseUnits_2 , activation = 'selu')(x)\n","\n","# Decoder\n","x = keras.layers.Dense(units=DenseUnits_1 , activation = 'selu')(x)\n","x = keras.layers.Dense(units=7040 , activation = 'selu')(x)\n","x = keras.layers.Reshape((5, 88, 16))(x)\n","x = keras.layers.Conv2DTranspose(16, kernel_size=(3,3), padding = 'same', activation = 'selu', strides = 2)(x)\n","x = keras.layers.Conv2DTranspose(8 , kernel_size=(3,3), padding = 'same', activation = 'selu', strides = 2)(x)\n","x = keras.layers.Conv2DTranspose(1 , kernel_size=(3,3), padding = 'same', activation = 'selu', strides = 2)(x)\n","\n","Conv_AE = Model(input, x)\n","\n","# Metric for optimization: Mean Absolute Error (MAE) = Reconstruction Error\n","Conv_AE.compile(loss=\"mae\", optimizer='adam', metrics=['mae'])\n","Conv_AE.summary()\n","\n","print('Latent space dimension : %d'%(DenseUnits_2))\n","################################################################################\n","\n","\n","# Conv_AE training\n","################################################################################\n","print(\"\\n↓↓↓↓↓ Start Conv_AE training ↓↓↓↓↓\\n\")\n","\n","t.tic() # training start time\n","\n","tf.random.set_seed(777)\n","history = Conv_AE.fit(TrainData, TrainData, epochs=Epoch, verbose=0,\n","                      validation_data=(TestData, TestData), callbacks=[MAE_PerEpoch()])\n","\n","t.toc() # training end time\n","time_s = t.tocvalue()\n","################################################################################\n","\n","# Evaluation accuracy with TestData based on trained model\n","Loss, Final_MAE = Conv_AE.evaluate(TestData,  TestData, verbose=0)\n","print(\"\\n[Final Epochs] MAE : %.4f\"%(Final_MAE))\n","print(\"Training time : %.4f seconds / %.4f seconds\"%(time_s, time_s/60))"]},{"cell_type":"code","source":["# Save the trained model\n","Conv_AE.save(\"/content/drive/MyDrive/ML10/ConvAE_LsDim_%d.h5\"%(DenseUnits_2))\n","\n","# Save the training history\n","Hist = pd.DataFrame(np.zeros((Epoch,2)))\n","Hist.iloc[:,0] = np.array(history.history['loss'])\n","Hist.iloc[:,1] = np.array(history.history['val_loss'])\n","Hist.to_csv(\"/content/drive/MyDrive/ML10/ConvAE_LsDim_%d_history.csv\"%(DenseUnits_2),header=None,index=None)"],"metadata":{"id":"RwxRyUQhYI3i"},"id":"RwxRyUQhYI3i","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compare Original Data vs Reconstructed Data"],"metadata":{"id":"gabJ5klwMJ8I"},"id":"gabJ5klwMJ8I"},{"cell_type":"code","source":["# Load the saved model\n","Conv_AE_load = keras.models.load_model(\"/content/drive/MyDrive/ML10/ConvAE_LsDim_%d.h5\"%(DenseUnits_2))"],"metadata":{"id":"PUIPzh3DbfiM"},"id":"PUIPzh3DbfiM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"61c6ab80","metadata":{"id":"61c6ab80"},"outputs":[],"source":["# Randomly select a data sample\n","datanum = np.random.randint(0,TestData.shape[0]-1)\n","\n","Original = TestData[datanum].reshape(width, height)\n","Restored = Conv_AE_load.predict(TestData[datanum:datanum+1]).reshape(width, height)\n","\n","print('Test data sample : %d'%(datanum))\n","\n","time = np.arange(0,  1+1/704, 1/703)\n","freq = np.arange(0, 24, 24/40)\n","\n","plt.figure(figsize=(10,4))\n","\n","plt.subplot(1,2,1)\n","plt.title('Orignal Data (Test_%d)'%(datanum))\n","plt.pcolormesh(time, freq, Original, cmap = 'summer')\n","plt.xlabel('Time (s)')\n","plt.ylabel('Frequency (kHz)')\n","plt.ylim([0,6])\n","\n","plt.subplot(1,2,2)\n","plt.title('Restored Data (Dim: %d)'%(DenseUnits_2))\n","plt.pcolormesh(time, freq, Restored, cmap = 'YlGn_r')\n","plt.xlabel('Time (s)')\n","plt.ylim([0,6])\n","\n","plt.show()"]},{"cell_type":"markdown","source":["# Extract Latent Space Features"],"metadata":{"id":"TjpdtTwya0lt"},"id":"TjpdtTwya0lt"},{"cell_type":"code","source":["from sklearn.decomposition import KernelPCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","import joblib"],"metadata":{"id":"0SjboBwYcLdd"},"id":"0SjboBwYcLdd","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3313282b","metadata":{"id":"3313282b"},"outputs":[],"source":["# Set ranges for each class\n","range_1_1, range_1_2 = sum(NoOfData.iloc[:0,0]), sum(NoOfData.iloc[:0,0])+NoOfData.iloc[0,0]\n","range_2_1, range_2_2 = sum(NoOfData.iloc[:1,0]), sum(NoOfData.iloc[:1,0])+NoOfData.iloc[1,0]\n","range_3_1, range_3_2 = sum(NoOfData.iloc[:2,0]), sum(NoOfData.iloc[:2,0])+NoOfData.iloc[2,0]\n","range_4_1, range_4_2 = sum(NoOfData.iloc[:3,0]), sum(NoOfData.iloc[:3,0])+NoOfData.iloc[3,0]\n","range_5_1, range_5_2 = sum(NoOfData.iloc[:4,0]), sum(NoOfData.iloc[:4,0])+NoOfData.iloc[4,0]\n","range_6_1, range_6_2 = sum(NoOfData.iloc[:5,0]), sum(NoOfData.iloc[:5,0])+NoOfData.iloc[5,0]"]},{"cell_type":"code","source":["# Set LS_dim (latent space's dimensionality)\n","LS_dim = DenseUnits_2\n","\n","# Search the bottle neck layer (= latent space)\n","x = pd.DataFrame(np.zeros((len(Conv_AE_load.layers), 2)), dtype=np.int64)\n","\n","for i, layer in enumerate(Conv_AE_load.layers):\n","\n","    temp_x = 1\n","    if i == 0:\n","        for j in range(1, len(Conv_AE_load.layers[i].output_shape[0])):\n","            temp_x = temp_x * layer.output_shape[0][j]\n","    else:\n","        for j in range(1, len(Conv_AE_load.layers[i].output_shape)):\n","            temp_x = temp_x * layer.output_shape[j]\n","\n","    x.iloc[i,0] = i\n","    x.iloc[i,1] = temp_x\n","\n","x_rank            = x.sort_values([1],ascending=True)\n","bottle_neck_layer = x_rank.iloc[0,0]\n","bottle_neck_dim   = x_rank.iloc[0,1]\n","layer_name        = Conv_AE_load.layers[bottle_neck_layer].name\n","\n","print('\\n\\n########################## \\n')\n","print('Latent space dimension : %d'%(LS_dim))\n","print('\\n########################## \\n\\n')\n","\n","print('bottle neck layer name : ' + layer_name)\n","print('bottle neck dimension  : %d'%(bottle_neck_dim))\n","\n","# Set a model that outputs both latent space features and final prediction\n","model        = Conv_AE_load\n","Visual_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n","\n","# Get latente space features from DataSet using Visaul_model\n","Latent_space = np.zeros((DataSet_rsp.shape[0] , bottle_neck_dim))\n","\n","for i in range(DataSet_rsp.shape[0]):\n","    temp_sample = DataSet_rsp[i:i+1]\n","    Ls_output, pred = Visual_model(temp_sample)\n","    Latent_space[i] = Ls_output\n","\n","# Scale the latent space features and save the scaler and features\n","scaler = MinMaxScaler()\n","Latent_space_std = scaler.fit_transform(Latent_space)\n","\n","VslModel_name = 'ConvAE_LatentDim_%d'%(LS_dim)\n","joblib.dump(scaler, \"/content/drive/MyDrive/ML10/scaler_\" + VslModel_name + \".save\")\n","\n","pd.DataFrame(Latent_space).to_csv('/content/drive/MyDrive/ML10/LS_output_D%d.csv'%(LS_dim), index=None)\n","pd.DataFrame(Latent_space_std).to_csv('/content/drive/MyDrive/ML10/LS_output_D%d_scaled.csv'%(LS_dim), index=None)\n","\n","# Print results\n","print('\\nLatent space feature (scaled) data shape :')\n","print(Latent_space_std.shape)"],"metadata":{"id":"XX-eRAgsbEaF"},"id":"XX-eRAgsbEaF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["Latent_space = pd.read_csv('/content/drive/MyDrive/ML10/LS_output_D%d.csv'%(LS_dim))\n","Latent_space"],"metadata":{"id":"9JWopOX0cROT"},"id":"9JWopOX0cROT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization of Latent Space Features by t-SNE"],"metadata":{"id":"uW8e1F0iNeIA"},"id":"uW8e1F0iNeIA"},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold      import TSNE\n","import time"],"metadata":{"id":"CTsxxzNXgD5D"},"id":"CTsxxzNXgD5D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### Load latant space features #######\n","print('\\n\\n########################## \\n')\n","print('Latent space dimension : %d'%(LS_dim))\n","print('\\n########################## \\n\\n')\n","\n","############ Execute t-SNE ##################\n","tsne         = TSNE(n_components=2, verbose=1, perplexity=100, n_iter=500, random_state=1)\n","tsne_results = tsne.fit_transform(Latent_space)\n","print('\\nt-SNE result data shape :')\n","print(tsne_results.shape)\n","\n","np.save(f'/content/drive/MyDrive/ML10/tSNE_D{LS_dim}.npy', tsne_results)"],"metadata":{"id":"CoEIc7hcczta"},"id":"CoEIc7hcczta","execution_count":null,"outputs":[]},{"cell_type":"code","source":["VslModel_name = 'ConvAE_LatentDim_%d'%(LS_dim)\n","\n","plt.figure(figsize=(7,5))\n","\n","plt.scatter(tsne_results[range_1_1:range_1_2,0], tsne_results[range_1_1:range_1_2,1], c='tab:red'    , label=cond[0], s=7)\n","plt.scatter(tsne_results[range_2_1:range_2_2,0], tsne_results[range_2_1:range_2_2,1], c='tab:orange' , label=cond[1], s=7)\n","plt.scatter(tsne_results[range_3_1:range_3_2,0], tsne_results[range_3_1:range_3_2,1], c='tab:green'  , label=cond[2], s=7)\n","plt.scatter(tsne_results[range_4_1:range_4_2,0], tsne_results[range_4_1:range_4_2,1], c='tab:cyan'   , label=cond[3], s=7)\n","plt.scatter(tsne_results[range_5_1:range_5_2,0], tsne_results[range_5_1:range_5_2,1], c='tab:blue'   , label=cond[4], s=7)\n","plt.scatter(tsne_results[range_6_1:range_6_2,0], tsne_results[range_6_1:range_6_2,1], c='tab:gray'   , label=cond[5], s=7)\n","\n","plt.title('Latent space (t-SNE) - ' + VslModel_name, fontsize=12)\n","plt.grid(alpha=0.3)\n","plt.legend(fontsize=8)\n","plt.xlabel('t-SNE_1')\n","plt.ylabel('t-SNE_2')\n","\n","fig = plt.gcf()\n","fig.savefig(\"/content/drive/MyDrive/ML10/tSNE_LSdim_%d.png\"%(LS_dim), dpi=fig.dpi)\n","\n","plt.show()"],"metadata":{"id":"Q1FOKEJsgtIe"},"id":"Q1FOKEJsgtIe","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}